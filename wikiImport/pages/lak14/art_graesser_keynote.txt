h1. Scaling Up Learning by Communicating with AutoTutor, Trialogs, and Pedagogical Agents
//Professor [[http://www.memphis.edu/mitsc/who-we-are/art-graesser.php|Art Graesser]]//
{{ pages:lak14_art_graesser_keynote02.png}}

Background in discourse processes - building computer agents to conversations with learners. 

h1. Agents
h2. Need to be sensitive to
  * Social
  * Cognitive
  * Affective
  * Motivational mechanisms?

h2. Roles
  * user-initiated help seeking
  * navigational guide (students don't know what to do next)
  * pairs of agents modeling action, thought, and social interaction
  * roles: peers, tutor, mentor, demagogue, adversarial

Poor at answering arbitrary questions, but only 5% of students ask questions actively. 

h2. Systems
{{ pages:lak14_art_graesser_keynote01.png}}

  * [[http://www.teachableagents.org/research/bettysbrain.php|Betty's Brain]]
  * [[http://www.autotutor.org/|AutoTutor]]
  * [[http://edgaps.org/gaps/projects/land-science/|LandScience]]
  * [[http://www.memphis.edu/psychology/vcaest/|VCAEST]]

h3. AutoTutor
h4. Managing turns
  * short feedback on previous turn
  * advance dialogue by one or more dialogue moves connected by discourse markers
  * end turn with a signal that transfers floor to student
    * question
    * prompting hand question etc
    * otherwise they just stare at each other "stand-off"

h2. Research
What do actual human tutors do. "[[http://psycnet.apa.org/index.cfm?fa=search.displayRecord&UID=2010-06038-019|Metaknowledge in tutoring]]", Graesser & Person (2009). 

h3. Strategies rarely used:
  * socratic tutoring (Collins, Stevens)
  * modeling-scaffolding-fading (Rogoff, Gardner)
  * reciprocal teaching (Brown, Palincsar)
  * building on pre-requisites (Gagne)
  * sophisticated motivational techniques (Lepper)
  * scaffolding SRL strategies (Azevedo, Winne)

h3. Tutor communication illusions (things they don't do well)
  * grounding
  * feedback accuracy (don't "round-reference"), usually positive feedback after major errors, because it's polite
  * discourse alignment, tutor throws out information, think that it's understood - it often isn't
  * student mastery
  * knowledge transfer

h3. Impact on learning
  * unskilled human tutors: .42 
  * AutoTutor .80
  * intelligent tutoring systems 1.0
  * skilled human tutors ??

h3. Track emotions during learning
  * boredom 23%
  * confusion 25% - best predictor of learning
  * delight 4%
  * flow 28%
  * frustration 16%
  * surprise 4%

Face at moment of confusion, and dialogue history, can help track emotions. Very difficult to differentiate between flow and boredom.

Best emotional attitude of agent differs with time.

h3. Trialogs
  * low ability -> vicarious learning
  * medium ability -> tutorial dialogue
  * high ability -> teachable agent

h3. Learning Conceptual Physics (VanLehn, Graesser 2007)
Conditions - test required conceptual understanding

  * read nothing (slightly higher than reading the textbook)
  * read textbook
  * AutoTutor (higher, about the same as human tutor)
  * human tutor

h2. Other presentations about conversational agents (AERA 11):
  * [[aera11:adcock]]
  * [[aera11:kim]]
  * [[aera11:veletsianos]]
  * [[aera11:overmyer]]
  * [[aera11:heller_procter]]
  * [[aera11:haake._silvervarg_et_al]]

::how are agents different from implicit affordances, embedded prompts, teacher-led scripts, etc?::