h1. Kindle notes

Knowledge  building  is  nevertheless  a  social  process.    Knowledge  building  has  been  described  in  terms  of  12  underlying  principles  (Scardamalia,  2002),  each  of  which  is  characterized  by  a  series  of  social  and  technological  determinants (loc: [[skimx://teplovs2010visualization#17|Page 17]])

Most  recently,  the  relationship  between  knowledge  building,  knowledge  building  environments,  and  21st  century  skills  has  been  examined  (Scardamalia,  Bransford,  Kozma,  &  Quellmalz,  2009) (loc: [[skimx://teplovs2010visualization#17|Page 17]])

This  echoes  the  take--home  message  of  Collins  and  Halverson  (2009):  that  schooling  must  be  reformed  to  meet  the  needs  of  society.    The  relationship  between  knowledge  building  and  21st  century  skills  can  be  elucidated  by  contrasting  a  "top-down"  approach  that  starts  with  identifying  goals  and  desired  outcomes  and  "working  backwards"  to  develop  implementation  strategies  to  a  "bottom--up"  or  "emergent"  one  in  which  goals  are  not  fixed  in  advance  but  rather  emerge  as  thinking  progresses.    Scardamalia  et  al.  (2009)  contrast  both  approaches  and  ultimately  suggest  integrating  the  two  models  through  a  series  of  investigations  including  "demonstrating how a broader systems perspective might inform large-scale, on-demand, summative assessment" (p.4, emphasis theirs) (loc: [[skimx://teplovs2010visualization#18|Page 18]])

The  study  of  social  networks  that  underlie  a  knowledge  building  community  is  a  worthwhile  and  important  endeavour,  and  at  least  two  recent  doctoral  dissertations    (Palonen,  2003;  Philip,  2009)  demonstrate  the  utility  of  doing (loc: [[skimx://teplovs2010visualization#18|Page 18]])

To  find  effective  measures  of  knowledge  building  we  need  to  contemplate  another  type  of  network  analysis  -  one  that  identifies  relationship  between  ideas,  as  well  as  between  people  contributing  those  ideas.    And  more  specifically,  we  must  be  able  to  identify  the  types  of  interactions  between  people  and  ideas  that  yield  knowledge  growth. (loc: [[skimx://teplovs2010visualization#19|Page 19]])

Because  ideas  are  the  central  objects  in  a  knowledge  building  community,  successful  communities  tend  to  generate  many  of  them.  This  perfusion  of  ideas  can  be  simultaneously  generative  and  paralysing.    It  is  generative  in  that  the  expression  of  ideas  often  generates  further  expression  of  ideas,  akin  to  "brainstorming".  It  can  be  paralysing  by  virtue  of  the  sheer  number  of  postings  that  must  be  read,  processed,  understood  and  assimilated.    Eliciting  blue-sky  scenarios  from  teachers  and  other  practitioners  struggling  with  this  sort  of  information  overload  include  the  desire  to  have  computers  help  them  make  sense  of  the  diversity  of  ideas  they  are  confronted  with. (loc: [[skimx://teplovs2010visualization#19|Page 19]])

n  The  Cambridge  Handbook  of  the  Learning  Sciences  (Sawyer,  2006),  knowledge  building  is  identified  as  one  of  five  foundations  of  the  learning  sciences.  It  is  characterized  by  six  themes  that  set  it  apart  from  other  educational  models:  1. Knowledge  advancement  as  a  community  rather  than  individual  achievement;  2. Knowledge  advancement  as  idea  improvement  rather  than  as  progress  toward  true  or  warranted  belief;  3. Knowledge  of  in  contrast  to  knowledge  about;  4. Discourse  as  collaborative  problem  solving  rather  than  as  argumentation;  5. Constructive  use  of  authoritative  information;  6. Understanding  as  an  emergent. (loc: [[skimx://teplovs2010visualization#22|Page 22]])

Bereiter and Scardamalia (2003) differentiate knowledge building from other constructivist approaches such as Learning by Design (Holbrook & Kolodner, 2000; Kolodner, 2002), Project-Based Science (Marx, Blumenfeld, Krajcik, & Soloway, 1997), and Problem-Based Learning by its focus on creative work with ideas. Scardamalia and Bereiter (2007) also differentiate it from A. L. Brown & Campione's (1994) Fostering Communities of Learners model. Knowledge building is a process of sustained idea improvement fostered by communities in which participants take responsibility for the advancement of community knowledge (Scardamalia, 2004).    Perhaps  most  important  is  the  notion  that  knowledge  building  concerns  itself  with  the  process  of  innovation  and  the  creation  of  new  knowledge.    Scardamalia  and  Bereiter  (2006)  describe  the  benefits  to  students  in  terms  of  a  cybernetic  system,  highlighting  the  fact  that  "the  main  value  is  this  epistemic  one  -  a  feedforward  effect,  in  which  new  knowledge  gives  rise  to  and  speeds  the  development  of  yet  newer  knowledge"  (p.  99).    They  hint  that  assessment  -  in  terms  of  judging  theories  and  models  generated  by  students  -  should  focus  on  the  generative  capacity  of  such  contributions  rather  than  conformity  to  the  status  quo. (loc: [[skimx://teplovs2010visualization#23|Page 23]])

Assessment  is  part  of  the  effort  to  advance  knowledge-it  is  used  to  identify  problems  as  the  work  proceeds  and  is  embedded  in  the  day--to--day  workings  of  the  organization.  The  community  engages  in  its  own  internal  assessment,  which  is  both  more  fine--tuned  and  rigorous  than  external  assessment,  and  serves  to  ensure  that  the  community's  work  will  exceed  the  expectations  of  external  assessors.  (p.12)  This  principle  has  been  extended  to  encompass  the  notion  of  concurrency:  that  is,  providing  feedback  to  the  users  in  real  time  (Chan  &  Lee,  2007;  Teplovs,  Donoahue,  Scardamalia,  &  Philip,  2007). (loc: [[skimx://teplovs2010visualization#24|Page 24]])

A  knowledge  building  environment  (KBE),  as  differentiated  from  a  knowledge  building  tool,  must  support  a  culture  of  creative  work  with  ideas.    A  KBE,  as  Scardamalia  (2003)  defines  it  is:  "any  environment  (virtual  or  otherwise)  that  enhances  collaborative  efforts  to  create  and    continually  improve  ideas." (loc: [[skimx://teplovs2010visualization#25|Page 25]])

Improvable  Ideas:  Knowledge  Forum  supports  recursion  in  all  aspects  of  its  design-there  is  always  a  higher  level,  there  is  always  opportunity  to  revise.  Background  operations  reflect  change:  continual  improvement,  revision,  theory  refinement.  There  are  at  least  two  types  of  "background  operations":  those  undertaken  by  participants  and  those  conducted  in  a  more  or  less  automated  fashion  by  computing  infrastructure.    Ideally  such  automated  background  operations  support  creative  work  with  ideas  without  quelling  the  willingness  of  participants  to  engage  in  risk-taking  with  ideas  (Scardamalia,  2003).    Recent  advances  in  information  visualization  suggest  that  we  now  have  available  the  necessary  computing  power  to  create  analytic  representations  that  can  be  used  by  non--experts.  Whereas  the  computing  power  may  be  available,  what  constitutes  a  useful  and  usable  visualization  is  not  clear. (loc: [[skimx://teplovs2010visualization#25|Page 25]]-26)

One  way  is  to  highlight  those  ideas  that  seem  related  or  linked  to  other  ideas  (and  are  therefore  potentially  more  useful  or  powerful).    Another  is  to  consider  the  visualizations  themselves  as  improvable  ideas. (loc: [[skimx://teplovs2010visualization#26|Page 26]])

Coherence-producing  mechanisms  for  dealing  with  information  overload  are  similarly  concerned  with  the  relationships  between  entries.    Ideas  can  be  "multiply  tagged,  linked,  referenced,  subordinated,  superordinated"  and  so  on  (Scardamalia  &  Bereiter,  1993,  p.  39). (loc: [[skimx://teplovs2010visualization#27|Page 27]])

One  of  Shneiderman's  (1996)  basic  tenets  of  information  visualization  is  the  need  to  provide  the  ability  to  zoom  and  filter.    This  maps  closely  onto  the  notion  of  "rise-above".    Rather  than  having  notes  subsumed  in  other  notes,  it  might  be  more  powerful  to  provide  a  sense  of  zooming  into  and  out  of  details  provided  by  risen-above  notes.    A  view--of--views  approach,  which  is  a  powerful  step  toward  rise--above  views,  is  a  good  first  approximation  at  the  ability  to  zoom  in  and  out  at  the  view  level  (Scardamalia  &  Bereiter,  2006) (loc: [[skimx://teplovs2010visualization#27|Page 27]]-28)

Storage  and  retrieval  for  situating  ideas  in  a  communal  context  (Scardamalia  &  Bereiter,  1993)  stresses  the  importance  of  placing  ideas  "in  the  context  of  entries  by  others"  (p.39).  Put  another  way,  this  feature  highlights  the  design  challenge  of  allowing  ideas  to  find  other  ideas. (loc: [[skimx://teplovs2010visualization#29|Page 29]])

Information  visualization  can  be  employed  to  understand  the  relationships  between  views,  either  within  the  same  database  or  across  apparently  disparate  databases.    It  can  be  used  to  present  areas  of  overlap  between  collections  of  notes.  Information  visualization  can  be  used  as  a  way  in  to  a  database  from  another  community  without  overwhelming  the  visitor. (loc: [[skimx://teplovs2010visualization#30|Page 30]])

Information  visualization  has  the  potential  to  facilitate  the  embedding  of  assessment  directly  in  the  knowledge  building  environment.    Benchmarks  can  be  included  in  the  discourse  environment  and  information  visualization  techniques  can  be  used  to  indicate  links  between  the  benchmarks  and  student  contributions.  Information  visualization  has  the  potential  to  allow  creative  assessment  of  groups  and  individual  processes,  contributions  rates  and  so  on. (loc: [[skimx://teplovs2010visualization#32|Page 32]])

Scardamalia  (2003)  enumerates  the  "knowledge  base  for  the  design  of  knowledge  building  environments"  by  drawing  on  dynamics  of  externalized,  organizational  memory  (Norman,  1993),  collective  cognitive  responsibility  for  knowledge  creation  (Scardamalia,  2002),  a  developmental  knowledge  building  trajectory  that  enables  the  link  between  learning  and  knowledge  creation,  intertextuality  (Bakhtin,  1986),  cultures  of  innovation  (Drucker,  1985),  the  objectification  of  knowledge  (Bereiter,  2002),  and  the  improvability  of  ideas  to  explain  anomalous  facts  (Lakatos,  1970) (loc: [[skimx://teplovs2010visualization#33|Page 33]])

 well--designed  information  visualizations  can  amplify  cognition  through  perception  (Card,  Mackinlay,  &  Shneiderman,  1999). (loc: [[skimx://teplovs2010visualization#33|Page 33]])

issue  of  scalability.    In  a  world  where  networks  or  communities  of  practice  are  becoming  commonplace,  how  can  one  judge  the  promisingness  of  within--  and  between--community  linkages,  particularly  given  the  near--infinite  number  of  such  links?    In  this  case,  using  information  visualization  techniques  based  on  content  can  lead  to  opportunistic  linking  among  and  between  communities.  However,  Scardamalia  (2003)  offers  an  admonishment  against  overuse  of  automated  content  analysis:    that  such  an  approach  may  curtail  risk--taking  with  ideas (loc: [[skimx://teplovs2010visualization#34|Page 34]])

Among  the  most  important  lessons  learned  from  several  decades  of  research  on  KBEs  are  the  following:  a  KBE  can  increase  opportunities  and  possibilities  for  knowledge  creation,  but  it  loses  that  capacity  when  it  becomes  overly  prescriptive.  It  must  provide  a  flexibility  consistent  with  the  emergent  goals  of  knowledge  creation.    It  should  not  use  prompts,  intelligent  agents,  prescribed  project,  fixed  task  sequences,  templates  or  other  means  to  guide  users  to  known  endpoints.    It  must,  instead,  capture  the  human  capacity  for  inventiveness  and  convert  that  inventiveness  into  something  of  social  value.  (Scardamalia,  2003,  p.    271) (loc: [[skimx://teplovs2010visualization#34|Page 34]]-35)

As  van  Aalst,  Sha,  and  Teplovs  (2010)  note,  interest  in  formative  assessment  was  piqued  after  Black  and  Wiliam  (1998)  published  a  major  review  of  the  field.      The  distinction  between  summative  and  formative  assessment  can  be  traced  to  an  early  seminal  paper  by  Scriven  (1967) (loc: [[skimx://teplovs2010visualization#35|Page 35]])

She  states  that  whereas  summative  assessment  can  exist  in  the  absence  of  formative  assessment,  the  converse  is  not  true.    That  is,  "formative  assessment  requires  summative  judgement  to  have  preceded  it"  (p.468),  even  if  the  summative  assessment  is  implicit.  This  represents  one  notion  of  formative  assessment.  A  more  expansive  concept  would  include  any  return  of  information  that  agents  would  find  helpful  in  improving  their  practices. (loc: [[skimx://teplovs2010visualization#36|Page 36]])

In  terms  of  knowledge  building,  formative  assessment  is  conceptualized  as  being  concurrent,  embedded  and  transformative  (Scardamalia,  2002) (loc: [[skimx://teplovs2010visualization#36|Page 36]])

Very  thorough  and  impressive  inquiry  thread  analyses  have  been  conducted  by,  for  example  Zhang,  Scardamalia,  Lamon,  Messina  and  Reeve  (2007) (loc: [[skimx://teplovs2010visualization#37|Page 37]])

Wasserman  and  Faust  (1997)  describe  social  network  analysis  as  a  methodology  that  focuses  on  relationships  and  patterns  of  relationships.    As  such  it  "requires  a  set  of  methods  and  analytic  concepts  that  are  distinct  from  the  methods  of  traditional  statistics  and  data  analysis"  (p.  3) (loc: [[skimx://teplovs2010visualization#37|Page 37]])

list  of  topics  that  have  been  studied  using  network  analytic  methods,  including  community  (Wellman,  1979),  group  problem  solving  (Bavelas,  1950;  Bavelas  &  Barrett,  1951;  Leavitt,  1951),  diffusion  and  adoption  of  innovations  (Coleman,  Katz,  &  Menzel,  1957,  1966;  Rogers,  1979),  and  cognition  (Freeman,  Romney,  &  Freeman,  1987;  Krackhardt,  1987). (loc: [[skimx://teplovs2010visualization#38|Page 38]])

A  problem  with  the  de  Laat  study  is  that  the  authors  equate  participation  with  learning.    That  is  not  entirely  unreasonable,  as  the  questions  they  were  interested  in  answering  were  less  about  the  nature  of  the  content  than  about  the  relationships  among  the  participants.    In  their  study  the  content  analysis  was  used  to  categorize  messages  as  either  "learning  processes"  or  "tutoring  processes",  but  the  relationship  among  ideas  within  those  messages  was  not  explored. (loc: [[skimx://teplovs2010visualization#39|Page 39]])

Brown  and  Duguid's  (2000)  Networks  of  Practice  (NoP)  model  represents  a  more  generalized  case  of  Lave  and  Wenger's  (1991)  Communities  of  Practice  model.    Individuals  in  a  Network  of  Practice  may  never  meet  each  other  but  are  connected  essentially  by  their  ideas  even  if  only  as  expressed  in  virtual  documents  such  as  electronic  mail.    In  a  NoP  individuals  share  knowledge  and  the  emergent  social  network  structure  is  determined  by  such  sharing. (loc: [[skimx://teplovs2010visualization#40|Page 40]])

As  with  Lave  and  Wenger,  Brown  and  Duguid  focus  on  knowledge  sharing  and  not  on  knowledge  construction.    Nevertheless  the  notion  that  information  can  take  on  a  social  life  of  its  own  is  an  important  one (loc: [[skimx://teplovs2010visualization#40|Page 40]])

Social  network  analysis  can,  and  has  been  conducted  on  the  social  dynamics  of  a  knowledge  building  community  (Philip,  2009).    But  there  is  another  equally  important  type  of  network  analysis  to  be  considered  in  the  case  of  knowledge  building:  the  network  of  ideas. (loc: [[skimx://teplovs2010visualization#40|Page 40]])

Latent  semantic  analysis  (LSA)  represents  both  a  statistical  technique  and  a  model  of  human  knowledge  acquisition.  Landauer  and  Dumais  (1997)  propose  LSA  as  a  model  that  could  provide  a  solution  to  the  question  of,  how  do  individuals  know  so  much  given  as  little  information  as  they  get? (loc: [[skimx://teplovs2010visualization#41|Page 41]])

LSA  provides  a  high--dimensional  (yet  still  reduced  in  dimensions  as  compared  with  "reality")  representation  of  the  associations  between  words  and  the  documents  containing  those  words.  The  final  output  from  LSA  is  a  series  of  measures  that  describe  the  relationships  between  words,  documents,  or  words--and--documents (loc: [[skimx://teplovs2010visualization#41|Page 41]])

The  Hyperspace  Analogue  of  Language  (HAL)  (Burgess,  Livesay,  &  Lund,  1998;  Burgess  &  Lund,  1997a,  1997b;  Lund  &  Burgess,  1997)  provides  an  alternative  vector  representation  of  memory.    It  is  similar  to  LSA  in  that  it  seeks  to  create  vector  representations  of  patterns  of  co--occurrences  of  words.    It  differs  chiefly  in  its  use  of  syntactic  elements  in  addition  to  purely  semantic  ones.    HAL  is  also  designed  to  derive  "sentential  meaning"  from  texts,  whereas  LSA  is  less  capable  of  doing  so  (Burgess,  et  al.,  1998). (loc: [[skimx://teplovs2010visualization#41|Page 41]])

Two  transformations  are  typically  applied  to  the  original  term--document  matrix:  a  local  "log"  transformation  and  a  global  "inverse  entropy"  transformation.  These  transformations  have  analogies  in  psychological  theories  of  learning  and  knowledge  acquisition.  The  contents  of  each  cell  in  the  term--document  matrix  are  replaced  with  the  logarithm  of  the  original  value  plus  one  (to  facilitate  those  cases  where  the  original  contents  are  zero).  The  local  (in  the  sense  of  being  applied  to  each  cell)  log  transformation  serves  to  amplify  the  relation  between  words  that  co--occur  in  multiple  contexts.  The  global  (in  the  sense  of  being  applied  across  the  contents  of  an  entire  row  in  the  term--document  matrix)  inverse  entropy  transformation  mimics  conditioning  effects  (Rescorla  &  Wagner,  1972).  It  puts  more  emphasis  on  words  that  are  not  evenly  distributed  across  contexts (loc: [[skimx://teplovs2010visualization#42|Page 42]])

The  subsequent  steps  of  LSA  are  not  represented  in  any  existing  theory  of  knowledge  acquisition.  The  singular  value  decomposition  (SVD)  and  dimension  reduction  serve  to  condense  the  original  matrix  into  something  that  represents  a  higher--order  (or  "latent")  semantic  structure.  It  is  this  latent  semantic  space  that  provides  a  representation  of  the  associations  between  stimuli  (i.e.  words  or  documents,  which  may  represent  specific  learning  episodes).  The  multi--dimensional  representation  of  stimuli  in  this  space  represents  a  quasi--stable  self--configured  system  analogous  to  physical  objects,  weather  systems,  ecosystems,  and  Hopfield  nets  (Hopfield,  1982). (loc: [[skimx://teplovs2010visualization#42|Page 42]])

Landauer  and  Dumais    (1997)  go  on  to  show  that  LSA  can  be  used  to  accurately  model  word--knowledge  acquisition  by  school--children,  and  how  LSA  can  provide  useful  insights  into  conditioning,  perceptual  learning,  chunking,  classical  association  theory,  analogs  to  episodic,  semantic,  explicit  and  implicit  memories,  the  origin  of  discrete  concepts  (or  words),  expertise,  and  contextual  disambiguation.    The  authors  also  explicate  the  relationship  between  LSA  and  Kintsch's  (1988)  Construction--Integration  (CI)  model  of  text  comprehension. (loc: [[skimx://teplovs2010visualization#42|Page 42]]-43)

73).  Kintsch  proposes  a  model  that  features  a  "knowledge  net",  which  consists  of  nodes  that  are  propositions,  schemas,  frames,  scripts,  and  production  rules.  The  links  between  the  nodes  form  an  associative  net,  and  the  "meaning"  of  a  node  is  determined  by  its  position  in  the  net.  This  definition  of  "meaning"  differs  from  the  psychological  definition  of  meaning,  which  in  this  model  can  be  conceptualized  as  the  nodes  in  the  net  that  are  activated  at  a  given  point  in  time.  The  number  of  nodes  that  are  active  is  limited  by  the  capacity  of  short--term  working  memory.  Hence,  the  "meaning"  of  a  node  will  be  relatively  consistent  over  time  but  may  vary  depending  on  which  nodes  have  been  activated.  Thus,  this  "knowledge  net"  representation  combines  features  of  episodic  and  semantic  memory  and  procedural  and  declarative  knowledge.  It  represents  a  radical  constructionist  (Bettencourt,  1993)  representation  of  knowledge,  since  the  meaning  of  any  particular  term  is  derived  solely  by  the  activation  and  proximity  of  other  nodes (loc: [[skimx://teplovs2010visualization#43|Page 43]])

The  representation  of  knowledge  as  a  knowledge  net  supports  the  notion  of  "emergent  structures"  since  they  are  associative  nets  that  change  based  on  perception  and  experience.  This  represents  a  fundamental  difference  between  this  representation  and  more  fixed  representations  such  as  scripts  and  frames  (Schank,  1982) (loc: [[skimx://teplovs2010visualization#44|Page 44]])

Note  that  Schank  (1982)  modified  the  script  notation  by  introducing  the  concept  of  Memory  Organization  Packets  (MOPs),  which  can  be  used  to  create  scripts  that  are  more  appropriate  to  specific  contexts.  Kintsch  and  Mannes  (1987)  have  shown  how  scripts  can  emerge  from  an  associating  knowledge  net.  Another  approach  to  determining  meaning  is  to  examine  the  semantic  relations  between  elements.    Collins  and  Quillian  (1969)  used  this  approach.    This  approach,  along  with  others,  proposes  a  model  of  the  organization  of  human  memory  (Minsky,  1975;  Quillian,  1966;  Rumelhart  &  Ortony,  1977;  Schank  &  Abelson,  1977;  Smith,  Shoben,  &  Rips,  1974). (loc: [[skimx://teplovs2010visualization#44|Page 44]])

Whereas  the  propositional  representation  of  knowledge  shows  tremendous  potential  in  educational  research,  its  use  is  hampered  by  the  lack  of  means  to  automatically  code  passages.  It  is  impossible,  for  example,  to  propositionalize  an  entire  textbook  or  a  protracted  (e.g.  year--long)  online  discussion.  Latent  Semantic  Analysis  (LSA)  may  provide  the  requisite  model  for  moving  propositional  analysis  beyond  the  brief  experiment.  LSA  provides  a  vector--based  equivalent  of  the  associative  net  discussed  above.  A  proposition  can  be  represented  in  high-dimensional  space  as  a  vector  of  numbers.  This  vector  contains  information  about  the  proposition's  relationship  with  other  propositions  in  the  knowledge  net.  The  dimension  reduction  that  is  performed  after  the  singular  value  decomposition  serves  to  eliminate  the  random  "noise"  -  the  accidental  misuses  of  words  or  details  of  the  discourse  space  that  are  largely  irrelevant  to  the  knowledge  it  contains.  This  is  not  to  say  that  LSA  provides  a  complete  model  of  human  cognition.  Indeed,  LSA's  "knowledge"  has  been  likened  to  a  "well--read  nun's  knowledge  of  sex"  (Landauer  et  al.,  1998,  p.  5).  LSA's  only  method  of  deriving  meaning  from  the  world  is  to  "read"  (or  mechanically  scan)  text  passages.  Nevertheless,  Kintsch  (1998)  closes  his  discussion  of  LSA  by  stating  that  "the  preliminary  results  that  are  available  at  this  point  suggest  a  great  potential  for  LSA  for  psychological  knowledge  representation"  (p.  92) (loc: [[skimx://teplovs2010visualization#44|Page 44]]-45)

In  a  study  that  investigated  matching  students  to  text,  Wolfe  et  al.  (1998)  assessed  study  participants  by  giving  them  tests  that  determined  their  knowledge  of  the  human  heart  and  the  circulatory  system.  Participants  were  then  assigned  one  of  four  readings  that  ranged  in  difficulty  from  elementary  school  level  to  medical  school  level.  Their  findings  demonstrated  that  participants  learned  best  when  the  material  they  read  was  neither  too  easy  nor  too  difficult,  given  their  current  level  of  understanding.  This  they  term  the  "Goldilocks"  principle  (similar  to  Vygotsky's  (1978)  Zone  of  Proximal  Development),  and  link  it  with  Kintsch's  (1994)  argument  that  learning  from  text  is  the  ability  to  link  it  with  previous  knowledge.  The  authors  cite  a  variety  of  works  that  demonstrate  that  learning  from  text  requires  appropriate  levels  of  prior  knowledge  (McKeown,  Beck,  Sinatra,  &  Loxterman,  1992;  Means  &  Voss,  1985;  Moravcsik  &  Kintsch,  1993;  Schneider,  K"orkel,  &  Weinert,  1990;  Spilich,  Vesonder,  Chiesi,  &  Voss,  1979) (loc: [[skimx://teplovs2010visualization#46|Page 46]])

Knowledge  Forum  consists  of  a  database  that  stores  participant--generated  ideas.    The  software  helps  users  organize  these  artifacts  into  collections  called  "views".    Views  consist  of  two--dimensional  Cartesian  planes  onto  which  iconic  representations  of  ideas  can  be  placed.    The  view  also  provides  a  background  onto  which  organizational  information  can  be  placed  using  graphics  or  text. (loc: [[skimx://teplovs2010visualization#53|Page 53]])

The  process  of  scaffolding  is  made  explicit  in  Knowledge  Forum  through  the  use  of  "thinking  types"  or  scaffold  supports  that  are  computer--mediated  and  customizable.    They  allow  teachers  and  students  to  use  scaffolds  and  rubrics  flexibly  and  for  students  to  tag  their  notes  by  thinking  type  (Andrade,  2000;  Chuy,  Scardamalia,  &  Bereiter,  2009;  Lai  &  Law,  2006;  Law  &  Wong,  2003) (loc: [[skimx://teplovs2010visualization#53|Page 53]])

A  limitation  of  the  current  system  of  scaffold  supports  that  is  available  in  Knowledge  Forum  is  that  the  scaffold  supports  are  only  available  for  use  at  the  note  authoring  level.    That  is,  they  are  designed  to  be  inserted  directly  into  notes.    There  is  no  facility  for  scaffolding  the  creation  of  views. (loc: [[skimx://teplovs2010visualization#54|Page 54]])

The  Student  ToolKit  (STK)  consists  of  two  components:  a  Student  ToolKit  for  queries  (STKq)  and  a  Student  ToolKit  for  visualizations  (STKv).    The  STKq  facilitates  the  retrieval  of  notes  that  match  criteria  such  as  "show  me  my  notes  that  overlap  with  curriculum  guidelines",  "show  me  my  growth  in  vocabulary",  and  "show  me  key  terms  and  phrases  in  the  curriculum  guidelines  that  are  not  represented  in  my  notes  to  date".    The  results  of  such  queries  can  be  fashioned  to  students  in  the  form  of  "proto--views"  that  the  students  can  then  work  up  into  views  that  they  author.    Alternatively,  the  temporary  proto--views  can  be  discarded,  as  may  be  appropriate  for  an  interim  assessment  of,  say,  vocabulary  growth (loc: [[skimx://teplovs2010visualization#62|Page 62]])

The  tools  and  techniques  developed  in  the  thesis  have  direct  impact  on  future  work  that  may  employ  methods  from  design--based  research.    Confrey  (2006)  traces  the  development  of  design--based  research  from  early  work  by  Piaget  (1976),  Vygotsky  (1978),  and  Dewey  (1981).    Design  researchers,  she  says  "make,  test,  and  refine  conjectures  about  the  learning  trajectory  based  on  evidence  as  they  go,  often  collaborating  with  or  acting  as  the  teachers,  and  assembling  extensive  records  on  what  students,  teachers,  and  researchers  learn  from  the  process.    They  then  conduct  further  analysis  after  the  fact  to  produce  research  reports  and/or  iterations  of  the  tasks,  materials,  and  instrumentation"  (p.  136).    She  highlights  the  early,  seminal  contributions  of  Collins  (1992)  and  Brown  (1992).    A  key  aspect  of  design  experiments  or  design  studies  is  the  provision  of  feedback  that  informs  design  iterations.    One  problem  with  the  provision  of  such  feedback  is  the  time--sensitive  nature  of  this  information.    Feedback  is  necessary  but  the  analyses  required  to  provide  meaningful,  intelligible  feedback  to  the  system  are  often  tedious  and  time-consuming  (Zhang,  et  al.,  2007).    Software  that  is  designed  to  assist  in  the  detection  of  patterns  and  trends  in  the  developmental  trajectories  may  serve  to  help  shorten  the  delay  between  iterations  and  therefore  may  facilitate  more  powerful  interventions.    Perhaps  more  importantly,  the  software  can  serve  to  help  design  researchers  detect  trends  and  anomalies  in  ways  that  were  previously  difficult  or  impossible  to  detect.    Thus  the  software  can  act  as  a  lens  onto  the  system  being  studied.    By  doing  so,  it  has  the  potential  to  mitigate  some  of  the  problems  associated  with  design  experimentation.    Specifically,  as  Collins,  Joseph  and  Bielaczyc  (2004)  explain,  design  experimentation:  involves  putting  a  first  version  of  a  design  into  the  world  to  see  how  it  works.    Then,  the  design  is  constantly  revised  based  on  experience....  Because  design  experiments  are  set  in  learning  environments,  there  are  many  variables  that  cannot  be  controlled.    Instead,  design  researchers  try  to  optimize  as  much  of  the  design  as  possible  and  to  observe  carefully  how  the  different  elements  are  working  out.  (p.18) (loc: [[skimx://teplovs2010visualization#68|Page 68]]-70)

The  design  of  the  Knowledge  Space  Visualizer  (KSV)  follows  Shneiderman's    (1996)  threefold  mantra  for  information  visualization:    (1)  provide  an  overview,  (2)  provide  the  ability  for  the  user  to  zoom  and  filter,  and  (3)  provide  details  on  demand (loc: [[skimx://teplovs2010visualization#71|Page 71]])

One  of  the  shortcomings  of  visual  analyses  is  that  they  tend  to  be  difficult  to  reproduce.    Screen  captures  are  an  effective  way  to  convey  the  resultant  images,  but  the  state  of  the  system  is  not  typically  saved.    The  KSV  works  in  concert  with  Knowledge  Forum  by  allowing  the  state  of  a  visualization  to  be  saved  to  the  database.    This  process  allows  researchers  to  share  their  visual  analyses  with  each  other,  which  in  turn  should  promote  collaborative  analysis  of  data  and  more  powerful  interpretations  of  the  data. (loc: [[skimx://teplovs2010visualization#80|Page 80]])

Whereas  terms  are  typically  scaled  according  to  their  frequency  of  occurrence,  there  is  no  need  to  think  that  is  the  optimal  representation  for  questions  about  semantic  fields.    Scaling  by  alternative  criteria  such  as  total  number  of  people  using  the  terms,  may  be  advantageous.  Hassan--Montero  and  Herrero--Solana  (2006)  have  suggested  a  number  of  different  ways  to  improve  tag  clouds  including  alternatives  to  the  usual  alphabetical  ordering  of  the  terms  in  the  cloud.      Term  clouds  are  capable  of  representing  higher  dimensional  data.    In  the  example  shown  in  Figure  42  the  intensity  of  the  font  colour  is  scaled  along  with  font  size.    It  may  be  advantageous  to  use  colour  to  indicate  another  dimension. (loc: [[skimx://teplovs2010visualization#135|Page 135]])

tag cloud (loc: [[skimx://teplovs2010visualization#135|Page 135]])

A  problem  with  term  clouds,  in  the  traditional  application  of  the  technique,  is  that  they  do  not  do  a  particularly  good  job  of  showing  semantic  field  growth.    There  are  at  least  two  reasons  for  this:      no  chronological  aspect,  and  no  sense  of  change  vs.  the  previous  term  cloud  (i.e.  a  sense  of  "delta").    The  use  of  font  sizing  to  scale  the  terms  in  the  cloud  is  also  questionable,    as  it  merely  highlights  frequently  used  terms  rather  than  highlighting  changes  over  time.      To  address  these  shortcomings,  a  new  graphical  form  of  tag  clouds  was  used.    Small  multiples  (Tufte,  1990)  were  used  to  present  a  chronology  of  semantic  field  changes.    In  this  technique,  the  term  clouds  reflect  the  changes  between  two  other  term  clouds,  and  the  term  counts  are  indicative  of  the  changes  in  term  frequency  rather  than  the  absolute  numbers.    Only  increases  in  term  counts  are  represented (loc: [[skimx://teplovs2010visualization#136|Page 136]])

Earl  (2004)  differentiates  among  "assessment  of  learning",  "assessment  for  learning"  and  "assessment  as  learning"  primarily  in  terms  of  who  the  recipient  of  the  assessment  result  is.    Assessment  of  learning  is  used  to  communicate  findings  of  student  performance  to  other  parties  (e.g.  parents)  and  to  justify  the  grades  that  are  assigned.    Assessment  for  learning  is  characterized  by  having  teachers  as  the  primary  recipients  of  the  assessment  results.    In  contrast  to  "assessment  of  learning"  students  are  seldom  compared  to  one  another.    Assessment  for  learning  represents  a  shift  from  summative  to  formative  assessment.    Assessment  as  learning  targets  the  students  as  recipients  of  the  feedback.    Students  are  regarded  as  "active,  engaged  and  critical  assessors  [who]  make  sense  of  information,  relate  it  to  prior  knowledge,  and  use  it  for  new  learning"  (p.  97). (loc: [[skimx://teplovs2010visualization#146|Page 146]]-147)

The  systems  architecture  model  calls  for  the  creation  of  user--generated  views  that  can  be  based  on  content--based  visualizations  such  as  those  demonstrating  overlap  with  curriculum  guidelines.  Such  demonstrations  of  coverage  (or  lack  thereof)  can  be  treated  as  artifacts  within  the  discourse  space  and  should  have  the  effect  of  creating  additional  discourse  around  them,  which  in  turn  could  be  the  source  material  for  further  visualizations. (loc: [[skimx://teplovs2010visualization#160|Page 160]])

Other  researchers  have  devised  advanced,  powerful,  but  time--consuming  techniques  for  the  analysis  of  threads  in  Knowledge  Forum.    For  example,  Zhang  et  al.  (2007)  propose  a  form  of  analysis  called  "inquiry  threads"  as  a  means  to  understand  the  socio--cognitive  dynamics  of  knowledge  building. (loc: [[skimx://teplovs2010visualization#164|Page 164]])

It  would  be  particularly  helpful  if  a  user  could,  while  looking  at  a  view,  be  able  to  ask  for  assistance  in  clustering  notes  and  receive  assistance  in  describing  the  resulting  clusters.    One  could  imagine  a  system  in  which  a  user  could  ask  for  a  view  to  be  organized  into  circular  clusters,  with  a  concise  tag  cloud  rendered  within  each  resultant  cluster. (loc: [[skimx://teplovs2010visualization#165|Page 165]]-166)

The  determination  of  coverage  can  be  done  either  through  explicit  semantic  links  (i.e.  referencing  the  curriculum  documents),  implicit  semantic  links  (i.e.  having  a  sufficient  number  of  LSA--based  note--to--note  cosines  that  exceed  some  threshold),  or  possibly  by  extending  the  knowledge  building  environment  to  allow  participants  to  mark  a  note  as  providing  evidence  of  curricular  coverage (loc: [[skimx://teplovs2010visualization#169|Page 169]]-170)

Some  preliminary  results  from  early  pilot  studies  suggests  that  users  experience  a  sense  of  wonder:  they  seem  engaged  by  the  fact  that  their  ideas  can  be  cast  in  a  more  global  context.  In  both  cases  people  appear  to  be  driven  by  a  sense  of  wonder.    In  the  case  of  researchers  it  is  typically  curiosity  about  the  system  that  they  are  studying.    In  the  case  of  participants  it  is  often  a  sense  of  wonder  about  oneself.    It  may  be  particularly  powerful  to  make  it  easy  for  students  and  teachers  to  display  the  semantic  space  around  a  particular  note.  Wonder,  then,  contributes  to  transformative  assessment.    It  is  not  externally  imposed  against  some  foreign  criterion,  but  rather  internally  generated.    Well-designed  and  well--implemented  assessment  tools  should  be  capable  of  generating  positive  feedback,  in  the  cybernetic  sense,  for  wonder  and  in  turn  both  depends  on  and  encourages  engagement  on  the  part  of  students (loc: [[skimx://teplovs2010visualization#172|Page 172]])

A  significant  improvement  to  Knowledge  Forum  would  be  the  inclusion  of  a  facility  to  allow  flexible  post  hoc  assignment  of  relationships  between  notes.    For  example,  one  could  indicate  that  one  note  "explains"  the  ideas  in  another  note.    Scaffolding  the  creation  of  links  that  indicate  the  relationships  between  notes  could  become  as  important  as  scaffolding  the  content  of  notes.      Once  the  relationships  between  notes  are  thus  identified,  visualization  techniques  can  be  applied  to  make  sense  of  the  resulting  patterns. (loc: [[skimx://teplovs2010visualization#179|Page 179]])

Collins  and  Halverson  (2009)  envision  a  future  in  which  technology  enables  people  of  all  ages  to  pursue  learning  on  their  own  terms.    In  his  review  of  their  book,  Grover  (2009)  notes  the  authors'  self--admitted  ambiguity  around  next  steps  and  implementation (loc: [[skimx://teplovs2010visualization#181|Page 181]])
